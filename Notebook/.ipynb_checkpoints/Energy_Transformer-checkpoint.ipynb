{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ro42b7SNwhjb",
    "outputId": "cc212822-1e72-447b-a5db-da8463d0e3cd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Assuming df has been defined and loaded with your data earlier\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "j9trE7Bkws6D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Timestamp  CAPITL  CENTRL  DUNWOD  GENESE    H Q  HUD VL  \\\n",
      "0       01/01/2000 00:00   42.88   39.49   43.70   38.98  39.98   42.92   \n",
      "1       01/01/2000 01:00   41.55   38.26   42.34   37.77  38.74   41.58   \n",
      "2       01/01/2000 02:00   40.98   37.74   41.76   37.25  38.20   41.01   \n",
      "3       01/01/2000 03:00   36.59   33.69   37.28   33.26  34.11   36.62   \n",
      "4       01/01/2000 04:00   42.88   39.49   43.70   38.98  39.98   42.92   \n",
      "...                  ...     ...     ...     ...     ...    ...     ...   \n",
      "222857  06/04/2025 19:00   59.72   57.66   60.76   56.99  55.75   60.25   \n",
      "222858  06/04/2025 20:00   58.57   57.73   59.75   57.56  55.74   59.41   \n",
      "222859  06/04/2025 21:00   50.16   48.17   49.49   47.88  47.53   49.30   \n",
      "222860  06/04/2025 22:00   41.36   40.40   42.16   40.00  39.93   41.84   \n",
      "222861  06/04/2025 23:00   36.23   35.46   37.07   35.39  34.70   36.89   \n",
      "\n",
      "        LONGIL  MHK VL  MILLWD  N.Y.C.  NORTH    NPX    O H    PJM   WEST  \n",
      "0        43.72   41.15   43.08   44.52  40.03  42.04  37.68  38.52  37.49  \n",
      "1        42.35   39.86   41.74   43.40  38.79  40.73  36.51  37.32  36.32  \n",
      "2        40.88   39.32   41.17   43.18  38.25  40.17  36.01  36.81  35.82  \n",
      "3        37.90   35.10   36.75   40.83  34.15  35.86  32.15  32.86  31.98  \n",
      "4        41.00   41.15   43.08   44.52  40.03  42.04  37.68  38.52  37.49  \n",
      "...        ...     ...     ...     ...    ...    ...    ...    ...    ...  \n",
      "222857   64.39   59.13   60.54   60.87  56.26  62.00  54.57  57.83  55.92  \n",
      "222858   63.00   59.08   59.64   59.81  56.54  59.34  55.06  57.20  56.54  \n",
      "222859   53.05   49.30   49.35   49.30  47.51  52.90  46.43  48.04  47.03  \n",
      "222860   46.72   41.44   42.00   42.40  40.13  42.12  38.64  40.40  39.32  \n",
      "222861   42.00   36.20   37.00   37.49  34.77  37.00  34.49  35.71  35.01  \n",
      "\n",
      "[222862 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# prompt: read all the csv files \"D:\\OneDrive - The Pennsylvania State University\\Research DATA\\Dr. Habib & Dr. Reza Data\\Energy Price Market Data\\Day Ahead Price Data_Processed\\USA\\NYISO\" and merge into a one dataframe df\n",
    "\n",
    "folder_path = r\"D:\\OneDrive - The Pennsylvania State University\\Research DATA\\Dr. Habib & Dr. Reza Data\\Energy Price Market Data\\Day Ahead Price Data_Processed\\USA\\NYISO\"\n",
    "\n",
    "all_nyiso_data = pd.DataFrame()\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                df_temp = pd.read_csv(file_path)\n",
    "                all_nyiso_data = pd.concat([all_nyiso_data, df_temp], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "df = all_nyiso_data.copy() # Assign the concatenated dataframe to df\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "u7f6rryGws8f",
    "outputId": "828e1dc9-c146-45ad-fc23-eb1e6555dfe4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>CAPITL</th>\n",
       "      <th>CENTRL</th>\n",
       "      <th>DUNWOD</th>\n",
       "      <th>GENESE</th>\n",
       "      <th>H Q</th>\n",
       "      <th>HUD VL</th>\n",
       "      <th>LONGIL</th>\n",
       "      <th>MHK VL</th>\n",
       "      <th>MILLWD</th>\n",
       "      <th>N.Y.C.</th>\n",
       "      <th>NORTH</th>\n",
       "      <th>NPX</th>\n",
       "      <th>O H</th>\n",
       "      <th>PJM</th>\n",
       "      <th>WEST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2000 00:00</td>\n",
       "      <td>42.88</td>\n",
       "      <td>39.49</td>\n",
       "      <td>43.70</td>\n",
       "      <td>38.98</td>\n",
       "      <td>39.98</td>\n",
       "      <td>42.92</td>\n",
       "      <td>43.72</td>\n",
       "      <td>41.15</td>\n",
       "      <td>43.08</td>\n",
       "      <td>44.52</td>\n",
       "      <td>40.03</td>\n",
       "      <td>42.04</td>\n",
       "      <td>37.68</td>\n",
       "      <td>38.52</td>\n",
       "      <td>37.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2000 01:00</td>\n",
       "      <td>41.55</td>\n",
       "      <td>38.26</td>\n",
       "      <td>42.34</td>\n",
       "      <td>37.77</td>\n",
       "      <td>38.74</td>\n",
       "      <td>41.58</td>\n",
       "      <td>42.35</td>\n",
       "      <td>39.86</td>\n",
       "      <td>41.74</td>\n",
       "      <td>43.40</td>\n",
       "      <td>38.79</td>\n",
       "      <td>40.73</td>\n",
       "      <td>36.51</td>\n",
       "      <td>37.32</td>\n",
       "      <td>36.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2000 02:00</td>\n",
       "      <td>40.98</td>\n",
       "      <td>37.74</td>\n",
       "      <td>41.76</td>\n",
       "      <td>37.25</td>\n",
       "      <td>38.20</td>\n",
       "      <td>41.01</td>\n",
       "      <td>40.88</td>\n",
       "      <td>39.32</td>\n",
       "      <td>41.17</td>\n",
       "      <td>43.18</td>\n",
       "      <td>38.25</td>\n",
       "      <td>40.17</td>\n",
       "      <td>36.01</td>\n",
       "      <td>36.81</td>\n",
       "      <td>35.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2000 03:00</td>\n",
       "      <td>36.59</td>\n",
       "      <td>33.69</td>\n",
       "      <td>37.28</td>\n",
       "      <td>33.26</td>\n",
       "      <td>34.11</td>\n",
       "      <td>36.62</td>\n",
       "      <td>37.90</td>\n",
       "      <td>35.10</td>\n",
       "      <td>36.75</td>\n",
       "      <td>40.83</td>\n",
       "      <td>34.15</td>\n",
       "      <td>35.86</td>\n",
       "      <td>32.15</td>\n",
       "      <td>32.86</td>\n",
       "      <td>31.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2000 04:00</td>\n",
       "      <td>42.88</td>\n",
       "      <td>39.49</td>\n",
       "      <td>43.70</td>\n",
       "      <td>38.98</td>\n",
       "      <td>39.98</td>\n",
       "      <td>42.92</td>\n",
       "      <td>41.00</td>\n",
       "      <td>41.15</td>\n",
       "      <td>43.08</td>\n",
       "      <td>44.52</td>\n",
       "      <td>40.03</td>\n",
       "      <td>42.04</td>\n",
       "      <td>37.68</td>\n",
       "      <td>38.52</td>\n",
       "      <td>37.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp  CAPITL  CENTRL  DUNWOD  GENESE    H Q  HUD VL  LONGIL  \\\n",
       "0  01/01/2000 00:00   42.88   39.49   43.70   38.98  39.98   42.92   43.72   \n",
       "1  01/01/2000 01:00   41.55   38.26   42.34   37.77  38.74   41.58   42.35   \n",
       "2  01/01/2000 02:00   40.98   37.74   41.76   37.25  38.20   41.01   40.88   \n",
       "3  01/01/2000 03:00   36.59   33.69   37.28   33.26  34.11   36.62   37.90   \n",
       "4  01/01/2000 04:00   42.88   39.49   43.70   38.98  39.98   42.92   41.00   \n",
       "\n",
       "   MHK VL  MILLWD  N.Y.C.  NORTH    NPX    O H    PJM   WEST  \n",
       "0   41.15   43.08   44.52  40.03  42.04  37.68  38.52  37.49  \n",
       "1   39.86   41.74   43.40  38.79  40.73  36.51  37.32  36.32  \n",
       "2   39.32   41.17   43.18  38.25  40.17  36.01  36.81  35.82  \n",
       "3   35.10   36.75   40.83  34.15  35.86  32.15  32.86  31.98  \n",
       "4   41.15   43.08   44.52  40.03  42.04  37.68  38.52  37.49  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (201601, 15)\n",
      "Test shape: (21261, 15)\n",
      "Train years: Index([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
      "       2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
      "      dtype='int32', name='Timestamp')\n",
      "Test years: Index([2023, 2024, 2025], dtype='int32', name='Timestamp')\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df['CAPITL'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X, y = [], []\n",
    "for i in range(60, len(scaled_data)):\n",
    "    X.append(scaled_data[i-60:i, 0])\n",
    "    y.append(scaled_data[i, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Splitting the data into training and testing sets based on the year\n",
    "train_df = df[df.index < \"2023-01-01\"]\n",
    "test_df = df[df.index >= \"2023-01-01\"]\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Train years:\", train_df.index.year.unique())\n",
    "print(\"Test years:\", test_df.index.year.unique())\n",
    "\n",
    "\n",
    "\n",
    "train_scaled = scaler.transform(train_df['CAPITL'].values.reshape(-1,1))\n",
    "test_scaled = scaler.transform(test_df['CAPITL'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uHRYdGzXw39T",
    "outputId": "74f873ce-0ce3-4443-db4f-61eead675912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m 523/6299\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08:06\u001b[0m 707ms/step - loss: 0.0236"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "for i in range(60, len(train_scaled)):\n",
    "    X_train.append(train_scaled[i-60:i, 0])\n",
    "    y_train.append(train_scaled[i, 0])\n",
    "for i in range(60, len(test_scaled)):\n",
    "    X_test.append(test_scaled[i-60:i, 0])\n",
    "    y_test.append(test_scaled[i, 0])\n",
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshaping for the Transformer model\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Transformer model definition\n",
    "# Include positional_encoding and transformer_encoder functions as defined\n",
    "\n",
    "def positional_encoding(positions, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    angle_rads = np.arange(positions)[:, np.newaxis] * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return x + res\n",
    "\n",
    "# Model building\n",
    "def build_transformer(time_steps, features, head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], dropout=0.1, mlp_dropout=0.1):\n",
    "    inputs = Input(shape=(time_steps, features))\n",
    "    x = inputs + positional_encoding(time_steps, features)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for units in mlp_units:\n",
    "        x = Dense(units, activation=\"relu\")(x)\n",
    "        x = Dropout(mlp_dropout)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Adjust the parameters based on your dataset specifics\n",
    "model = build_transformer(\n",
    "    time_steps=X_train.shape[1],\n",
    "    features=X_train.shape[2],\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    dropout=0.1,\n",
    "    mlp_dropout=0.1\n",
    ")\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "predicted_prices = model.predict(X_test)\n",
    "predicted_prices = scaler.inverse_transform(predicted_prices)\n",
    "\n",
    "# Evaluation\n",
    "test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "mse = mean_squared_error(test_actual, predicted_prices)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_actual, predicted_prices)\n",
    "r2 = r2_score(test_actual, predicted_prices)\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0  # handle the case where both prediction and actual are zero\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "smape = calculate_smape(test_actual, predicted_prices)\n",
    "\n",
    "print(f\"Transformer Model Evaluation Metrics:\\nMAE: {mae}\\nRMSE: {rmse}\\nsMAPE: {smape}%\\nR^2: {r2}\")\n",
    "\n",
    "# Visualization\n",
    "test_df_index_truncated = test_df.index[60:]  # Adjust to match the length of predictions\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(test_df_index_truncated, test_actual, label='Actual Price')\n",
    "plt.plot(test_df_index_truncated, predicted_prices, label='Predicted Price')\n",
    "plt.title('Price Prediction for 2023')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=30))\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "94PRm4wP-jR7",
    "outputId": "769aaf3f-fe60-4b5c-8c90-a6aa34a86c9f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss values from the history object\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "uJpdi0eh-tWE",
    "outputId": "bcd3a747-354b-4302-896e-c2d72b5cbb90"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ovkov64o-5f-",
    "outputId": "8f52524a-493f-44ee-9144-89c0e9fe2edc"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predicted_prices = model.predict(X_test)\n",
    "predicted_prices = scaler.inverse_transform(predicted_prices)\n",
    "test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = test_actual - predicted_prices\n",
    "\n",
    "# Plotting residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=25, alpha=0.75, edgecolor='black')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=test_actual, y=residuals, alpha=0.75)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title('Residuals vs. Actual Prices')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmhXVWs2-xYs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(test_actual, predicted_prices)\n",
    "print(f'R2 Score: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tjit0GNW_ge0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_indices = y_true != 0  # Filter out zero values to avoid division by zero\n",
    "    y_true_filtered, y_pred_filtered = y_true[non_zero_indices], y_pred[non_zero_indices]\n",
    "    mape = np.mean(np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)) * 100\n",
    "    return mape\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "# Assuming you have test_actual and predicted_prices from your previous code\n",
    "mape_value = calculate_mape(test_actual, predicted_prices)\n",
    "r2_value = calculate_r2(test_actual, predicted_prices)\n",
    "\n",
    "print(f\"MAPE: {mape_value}%\")\n",
    "print(f\"R-squared Value: {r2_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAuWAI_E_wWE"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for the predicted values\n",
    "predicted_df = pd.DataFrame(predicted_prices, index=test_df_index_truncated, columns=['Predicted'])\n",
    "\n",
    "# Since test_actual_truncated is a NumPy array, convert it back to a DataFrame\n",
    "actual_df = pd.DataFrame(test_actual_truncated, index=test_df_index_truncated, columns=['Actual'])\n",
    "\n",
    "# # Combine actual and predicted values into a single DataFrame for comparison\n",
    "comparison_df = pd.concat([actual_df, predicted_df], axis=1)\n",
    "\n",
    "# # Save the DataFrame to an Excel file\n",
    "comparison_df.to_excel('model_predictions by transformer.xlsx')\n",
    "\n",
    "# print(\"Excel file created: model_predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-_g4rxM584x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7mhYH0l59qn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
